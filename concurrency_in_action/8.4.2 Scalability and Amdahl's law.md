Scalability is all about ensuring that an application can take advantage of additional processors in the system it’s running on.

Every time one thread has to wait for something, whatever that something may be, if there are no ready threads waiting to take it’s place on the processor, you have a processor sitting idle that could be doing useful work.

A simplified way of looking at this is to divide a program into “serial” sections, in which only one thread is doing any useful work and “parallel” section where all available processors are doing useful work. If one runs this application on a system with more processors, the “parallel” sections would in theory be able to complete more quickly, because work can be divided between more processors whereas the serial sections will remain serial.

Under such a simplified set of assumptions, one can estimate the potential performance gain to be achieved by increasing the number of processors; if the “serial” sections constitute a fraction, $f_s$ of the program, then the performance gain, $P$ , from using $N$ processors can be estimated as:
$$
P=\frac{1}{f_s + \dfrac{1-f_s}{N}}
$$
This is $Amdahl’s\space law$, It is often cited when talking about performance of concurrent code. If everything could be parallelized meaning $f_s$, the serial fraction is 0, the speedup therefore is $N$. Alternatively, if the serial fraction is one-third, even with an infinite number of processors you’re not going to get a speedup of more than 3.

A way to think of the serial fraction would be as the ratio of currently executable tasks on a processor (which is usually 1 on single cores) to the total number of tasks on that processor, at least that’s how i understand. Meaning if the serial fraction is 1 ergo all tasks on that processor can all simultaneously be executed at the same time with none left waiting, the operation is no longer serial making the serial fraction 0. ==Remember (while re-reading) to check the exactitude of this claim==.

An axiom clear with amdahl’s law is; when using concurrency for performance, it is beneficial to ensure that there’s always useful work for the processors to be doing. If one can reduce the size of the serial fraction or reduce the potential for threads to wait, the potential for performance gains on systems with more processors will increase.

Scalability is about reducing the time taken to perform an action or increase the amount of data that can be processed in a given time as more processors are added. Sometimes both are synonymous because you can process more data if each element is processed faster.

### Hiding latency with multiple threads
In multi-threaded application code, threads frequently block while waiting for something, (I/O, mutex acquisition, waiting on a conditional variable, a future or just sleeping for a period of time).

Whatever is the reason for a wait, if one has only as many threads as there physical processing units, having blocked threads means CPU time is being wasted on nothing. The processor that would otherwise be running a blocked threads is instead going nothing. Consequently if one knows that it’s thread will be waiting for a considerable amount of time, That spare CPU time can be put in use by running one or more additional threads.

This list highlight guidelines to maximize concurrency potential and hide latency in situations where threads could be waiting, it should be noted that situations where I label “I/O bound” in this listing can be replaced with any task that waits for something:
1)  If one has an application that operates using a multi-threading pipeline, if a thread in the pipeline is I/O bound, hence must wait for something at one point or another, the spare CPU time should be made use of by running additional threads that encapsulate the main business logic of that application, using a virus scanning application as an example, it’s file searching thread that (hypothetically) places files to be scanned on a queue will be I/O bound while the main business logic thread will threads that perform the actual virus-scanning, so in cases where there are spare CPU time; more “actual virus-scanning” threads should be spawned. This should be done carefully with measurements.
2) Depending on the application it can be possible to utilize spare CPU time without running additional threads. For example, if a thread is blocked due to I/O, asynchronous I/O should be used instead (if available) allowing said thread to perform other useful work while I/O is performed in the background
3) In another case if a thread is waiting for another thread to perform an operation, rather than blocking, the waiting thread might be able to perform that operation  itself.
4) In extreme cases, if a thread is waiting for a task to be completed and that task has not yet been started by any thread, the waiting thread might perform the task in entirety itself. Just like the example [Here](8.0%20Designing%20Concurrent%20Code.md#Dividing%20data%20recursively).

### Improving responsiveness with concurrency
