### Waiting for the first future in a set with `when_any`
Suppose we are searching  large data set for value that meets a specific criteria, but if there are multiple of such value, then any will do. 

An approach to implement this would be to, slice the entire data set into smaller subsets, perform the search on each subset with each search running in parallel with each other, reserve a shared boolean flag to signal if a match has been found, then in the event that a match has been found by some thread, the thread updates the flag causing other searches to terminate their execution. 

To achieve this we can use `std::experimental::when_any`, `when_any` gathers futures together and provides a new future that is ready when at least on of the original set is ready

`when_all` returns, as a future, collection of all passed in futures now in their ready state, meanwhile `when_any` adds a layer to that; it returns the collection plus the index of the first future recognized to be ready this amalgam is stored as an instance of `std::experimental::when_any_result` wrapped in a future.

An example illustrating `when_any` is given below.
##### using `std::experimental::when_any` to process the first value found
```c++
std::experimental::future<FinalResult>
	find_and_process_value(std::vector<MyData>& data)
{
	unsigned const concurrency=std::thread::hardware_concurrency();
	unsigned const num_tasks=concurrency > 0 ? concurrency : 2;
	std::vector<std::experimental::future<*MyData>> results;
	
	auto const chunk_size = data.size() + num_tasks - 1 / num_tasks;
	auto chunk_begin = data.begin();
	
	std::shared_ptr<atomic<bool>> done_flag = 
		std::make_shared<std::atomic<bool>> (false);
	
	for (unsigned i=0; i<num_tasks; ++i) {
		auto chunk_end = 
			(i < (num_tasks-1)) ? chunk_begin+chunk_size : data.end();
		result.push_back(spawn_async([=]{
			for (auto entry = chunk_begin; !*done_flag && 
				(chunk_begin != chunk_end); ++entry)
			{
				if (matches_find_criteria(*entry)) {
					*done_flag = true;
					return &*entry;
				}
			}
			return (*MyData)nullptr;
		}));
		chunk_begin = chunk_end;
	}
	
	std::shared_ptr<std::experimental::promise<FinalResult>> final_result
		= std::make_shared<std::experimental::promise<FinalResult>>();
	
	struct DoneCheck {
		std::shared_ptr<std::experimental::promise<FinalResult>>
			final_result;
			
		DoneCheck(
			std::shared_ptr<std::experimental::promise<FinalResult>>
			final_result_)
			: final_result(std::move(final_result_))
			{}
		
		void operator()(
		std::experimental::future<std::experimental::when_any_result<
		std::vector<std::experimental::future<*MyData>>>> result_param)
		{
			auto results = result_param.get();
			MyData* const ready_res =
				results.futures[results.index].get();
				
			if (ready_res)
				final_result->set_value(process_found_value(*ready_res));
			else {
				results.futures.erase(results.futures.begin()+
					results.index);
				if(!results.futures.empty()) {
					std::experimental::when_any(
						results.futures.begin(), results.futures.end())
						.then(std::move(*this));
				} else {
					final_result->set_exception(
						std::make_exception_ptr(
							std::runtime_error("Not found")));
				}
			}
		}
	};
	
	std::experimental::when_any(results.begin(), results.end())
		.then(DoneCheck(final_result));
	return final_result->get_future();
}
```
This example, although a mouthful, does a nice job illustrating the capabilities of `when_any`. If confused refer to the text.

Both `when_all` and `when_any` have overloads that accept range denoting iterators to define the set of futures they should wait for. 

Both functions also come in variadic form so an arbitrary number of futures can be directly passed into them, In this case, the result is a future holding a tuple (or a `when_any_result` holding a tuple)  rather than a vector:
```c++
std::experimental::future<int> f1=spawn_async(func1);
std::experimental::future<std::string> f2=spawn_async(func2);
std::experimental::future<double> f3=spawn_async(func3);
std::experimental::future<
	std::tuple<
		std::experimental::future<int>,
		std::experimental::future<std::string>,
		std::experimental::future<double> >> result = std::experimental::when_all(std::move(f1), std::move(f2), std::move(f3));
```
`when_all` and `when_any` takes in future arguments passed into them by value, so we have to explicitly move futures in or pass in temporaries (rvalues).

### Latches and barriers in Concurrency TS
A latch is a synchronization object that becomes *ready* once it’s counter is decremented to zero. It’s name comes from the fact that it latches the output—meaning once it’s ready it remains ready until destroyed.

A barrier on the other hand, is a reusable synchronization component used by a group of threads, internally, to enforce/uphold their synchronized state.

A latch doesn’t care which thread and by how much that thread decrements it’s counter.

A barrier, on the other hand, models it’s synchronization mechanism as a cyclic operation, a thread can only arrive at that barrier once per cycle. When a thread reaches a barrier it blocks until all other threads involved has arrived at the barrier, at which point they are all released. Th barrier can then be reused for the next cycle.

### A basic latch type `std::experimental::latch`
`std::experimental::latch` comes from the `<experimental/latch>` header.  A latch takes in the number of events to be waited for as it’s only arguments when constructing.

Events interested in partaking in the synchronization take on the obligation of invoking the latches`count_down()` member function signifying that **it**, as an event, has completed it’s execution, hence “has occurred”. 

Once all events partaking in the synchronization (basically the *wait* synchronization) successfully decrements the latches counter to zero, the latch becomes ready and then the synchronized event, the event that had been waiting on the latch to become ready, can now be scheduled to execute.
```c++
void foo()
{
	unsigned const thread_count=whatever;
	latch done (thread_count);
	my_data data[thread_count];
	std::vector<std::future<void>> threads;
	for (unsigned i=0; i<thread_count; ++i)
		threads.push_back(std::async(std::launch::async, [&, i]{
			data[i]=make_data(i);
			done.count_down();
			do_more_stuff();
		}));
	done.wait(); // latch waiting for its counter to be zeroed
	process_data(data, thread_count); // synchronized event
}
```
In the lambda passed into `std::async` all identifiers are captured by reference except `i`, `i` is not captured by reference but by value because by doing the former, data races could occur; imagine we capture `i` by reference, then spawn a new task meaning: returning to the calling loop to iterate  to its next index and spawn another new task, in the space within when the first task was spawn, at that tasks execution at `data[i]` , the calling loop (running in the background) could have leapt to the next iteration and updated `i` for the next task spawn, and all this could happen, gingerly,  within the space the previously spawned task executed `data[i]` and (just) right before `make_data[i]`  of that same previous task is executed, This would mean the `i` in both executions of the same thread will posses different values; a classic data race.

### `std::experimental::barrier` a basic barrier
Concurrency TS provides two types of barriers in `<experimental/barriers>`; `std::experimental::barrier` and `std::experimental::flex_barrier`. Former is more basic, hence lower overhead. The latter is more flexible, hence more overhead.

Suppose we have a group of threads that are operating on some data, each threads can do its processing independent of others so no synchronization is needed for that; but all threads must have had processed that data, before the next data can be scheduled to process; a kind of cyclic processing in which all stationed threads must have processed a single data entity before the next data entity can start its own processing.

Such cyclically-processing mechanisms can be modeled in concurrent programs using barriers in c++.

A barrier is constructed by specifying the number of threads involved in the synchronization group. As each thread is done with it’s processing it arrives at the barrier an waits for the rest of the group by invoking `arrive_and_wait` on the barrier object, Once all participating threads arrives after successively waiting (except the last thread i guess), the threads are released, the barrier is reset and a new task is fed to the group and the processing restarts.

The task could be a combined-effort processing on a data sequence; combined effort in the sense that the collective effect each thread in the thread group has on data unit is required to process that data.

Or the task could be a stage in processing; all threads could handle some state; with the same argument fed to each states, the collective resultant can be different compared to if a different argument was fed. kind of like a robot, in an obscure way.

Barriers are reusable, making them cyclic in nature. They also synchronize within a group of threads meaning a thread cannot wait for a barrier to be ready unless it’s part of the synchronization group.

Threads can explicitly drop out of a group by invoking `arrive_and_drop`on the barrier meaning after that the thread cannot wait on the barrier anymore.

An example of `std::experimental::barrier`
```c++
result_chunk process(data_chunk);
std::vector<data_ckunk> 
divide_into_chunks(data_block data, unsigned num_threads);

void process_data(data_source &source, data_sink &sink)
{
	unsigned const concurrency = std::thread::hardware_concurrency();
	unsigned const num_threads = concurrency > 0 ? concurrency : 2;
	
	std::experimental::barrier sync (num_threads);
	std::vector<joining_thread> threads (num_threads);
	
	std::vector<data_chunk> chunks;
	result_block result;
	
	for (unsigned i = 0; i < num_threads; ++i){
		threads[i] = joining_thread([&, i]{
			while (!source.done()) { //6
				if(!i) { //1
					data_block current_block = 
						source.get_next_data_block();
					chunks = divide_into_chunks(
						current_bloack, num_threads);
				}
				sync.arrive_and_wait(); //2
				result.set_chunk(i, num_thread, process(chunks[i])); //3
				sync.arrive_and_wait(); //4
				if(!i) { //5
					sink.write_data(std::move(result));
				}
			}
		});
	}
} //7
```
code explained in text.

### `std::experimental::flex_barrier`— barriers flexible variant.
Man fuck this section i’m done.