This section explains that when designing concurrent access to data structures the the structure should be holistically pondered upon to discover areas where potential for concurrency can be maximized.

Like in the case of multiplying  two large matrices, there would exist three matrices, the first and second source matrices, and the matrix in which the result will be stored. if we designate each threads to compute the resultant for a subset of the contiguous columns, each thread would have to access $N$(where $N$ is the number of columns) number of elements in each row of one of the matrices plus all the elements in the other matrix, meaning numerous cache-lines to provide those data firstly which is mostly fine and required, there there is higher risk of false sharing and latency in data writing due to accessing a subset of the contingent data of a single row in the resultant matrix and this is done for multiple rows.

It is then discovered that subjecting the threads to compute a subset resultant rows instead of columns proves to be more efficient due to the fact that when writing to the resultant matrix the cache-line is sure to carry contingent data that is sure to be utilized by the thread since rows are array which are inherently contingent; significantly reducing the possibilities of false sharing plus if the size the cache-line matches that of the size of the row array to be written the risk of false sharing is further reduced aggressively.

Further investigation then reveals that assigning each thread to compute a rectangular subset, of the matrix leads to less elements being read which in turn also boosts performance by reducing the possibilities of false sharing (in the read side) since there are less elements being accessed; for example, say we have a hardware of 100 processors for the multiplication of two matrices; $1000\times1000$ in size, if we followed the previous approach of designating each thread to compute a subset of rows/columns for the resultant matrix; say each thread is to compute 10 rows/columns, that means for the first matrix there would be $10\times1000 \space elements$ which is equal to $10,000\space elements$ on the first matrix read and since the same thread need to access all elements on the second matrix its reads there are $1,000,000\space elements$ making the total elements it would access $1,010,000\space elements$  just to compute $10,000\space resultant \space elements$.

But in the rectangular subset approach if we designate each thread to compute the resultant for each $100\times100$ square subset of the resultant matrix, access would be $100\times1000\space reads$ one the first source matrix and the same on the second source matrix, making it $100,000\space  elements$ accessed on the first matrix and the same on the second; a total of $200,000\space elements$ accessed to yield the same $10,000\space resultant\space elements$ , that’s a 5 fold reduction in element reads.

So it’s important to test data structures with different computation and data access profiles to discover which profile best maximizes the concurrency potential of the hardware for the program.

### Data access patterns in other data structures
Fundamentally the same considerations apply when trying to optimize data access patterns of other data structures:
* Try to adjust data distribution between threads so that data that’s close together is worked on by the same thread
* Try to minimize the data required by any given thread
* Try to ensure data accessed by different threads is sufficiently far apart to avoid false sharing via `std::hardware_destructive_inteference_size` as a guide.

the rest of this section explains how even while locking mutexes, data proximity between the mutex and the data it protects can result to performance hit to the lock owning thread even in situations when another thread unsuccessfully tries to lock the mutex;  just performing the action of request for a mutex, can demand that the cache-line holding the mutex, along with the protected data, be transported to the requesting thread leaving the reference of the protected data in the owning thread invalidated, this issue can be ameliorated by placing byte paddings the size of the cache-line in-between mutexes and it’s protected data, in order to avoid false sharing of the protected data:
```c++
struct protected_data
{
	std::mutex m;
	char padding {std::hardware_destructive_interference_size};
	my_data data_to_ptotect;
};
```
if this improves performance then false sharing was the culprit.

## 8.4 Additional consideration when designing for concurrency
There more to designing concurrency code than how to divide work between threads, factors that affect performance and how data access patterns to data structures can affect concurrency.

We also need to consider things such as exception safety and scalability. Code is said to be scalable if the performance (whether in terms of reduced speed of execution or throughput) increases as more processing cores are added to the system. Ideally the performance increase is linear.

### Exception Safety in parallel algorithms
Concurrent applications need to exception proof in it’s design, this is because if a function spawned on a new thread exits with an exception, the application is terminated.

Here is a naive implementation of `std::accumulate` in which we will use to address several design concerns related to exceptions:
```c++

```