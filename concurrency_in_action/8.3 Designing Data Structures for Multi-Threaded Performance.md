This section explains that when designing concurrent access to data structures the the structure should be holistically pondered upon to discover areas where potential for concurrency can be maximized.

Like in the case of multiplying  two large matrices, there would exist three matrices, the first and second source matrices, and the matrix in which the result will be stored. if we designate each threads to compute the resultant for a subset of the contiguous columns, each thread would have to access $N$(where $N$ is the number of columns) number of elements in each row of one of the matrices plus all the elements in the other matrix, meaning numerous cache-lines to provide those data firstly which is mostly fine and required, there there is higher risk of false sharing and latency in data writing due to accessing a subset of the contingent data of a single row in the resultant matrix and this is done for multiple rows.

It is then discovered that subjecting the threads to compute a subset resultant rows instead of columns proves to be more efficient due to the fact that when writing to the resultant matrix the cache-line is sure to carry contingent data that is sure to be utilized by the thread since rows are array which are inherently contingent; significantly reducing the possibilities of false sharing plus if the size the cache-line matches that of the size of the row array to be written the risk of false sharing is further reduced aggressively.

Further investigation then reveals that assigning each thread to compute a rectangular subset, of the matrix leads to less elements being read which in turn also boosts performance by reducing the possibilities of false sharing (in the read side) since there are less elements being accessed; for example, say we have a hardware of 100 processors for the multiplication of two matrices; $1000\times1000$ in size, if we followed the previous approach of designating each thread to compute a subset of rows/columns for the resultant matrix; say each thread is to compute 10 rows/columns, that means for the first matrix there would be $10\times1000 \space elements$ which is equal to $10,000\space elements$ on the first matrix read and since the same thread need to access all elements on the second matrix its reads there are $1,000,000\space elements$ making the total elements it would access $1,010,000\space elements$  just to compute $10,000\space resultant \space elements$.

But in the rectangular subset approach if we designate each thread to compute the resultant for each $100\times100$ square subset of the resultant matrix, access would be $100\times1000\space reads$ one the first source matrix and the same on the second source matrix, making it $100,000\space  elements$ accessed on the first matrix and the same on the second; a total of $200,000\space elements$ accessed to yield the same $10,000\space resultant\space elements$ , that’s a 5 fold reduction in element reads.

So it’s important to test data structures with different computation and data access profiles to discover which profile best maximizes the concurrency potential of the hardware for the program.

### Data access patterns in other data structures
Fundamentally the same considerations apply when trying to optimize data access patterns of other data structures:
* Try to adjust data distribution between threads so that data that’s close together is worked on by the same thread
* Try to minimize the data required by any given thread
* Try to ensure data accessed by different threads is sufficiently far apart to avoid false sharing via `std::hardware_destructive_inteference_size` as a guide.

the rest of this section explains how even while locking mutexes, data proximity between the mutex and the data it protects can result to performance hit to the lock owning thread even in situations when another thread unsuccessfully tries to lock the mutex;  just performing the action of request for a mutex, can demand that the cache-line holding the mutex, along with the protected data, be transported to the requesting thread leaving the reference of the protected data in the owning thread invalidated, this issue can be ameliorated by placing byte paddings the size of the cache-line in-between mutexes and it’s protected data, in order to avoid false sharing of the protected data:
```c++
struct protected_data
{
	std::mutex m;
	char padding {std::hardware_destructive_interference_size};
	my_data data_to_ptotect;
};
```
if this improves performance then false sharing was the culprit.

## 8.4 Additional consideration when designing for concurrency
There more to designing concurrency code than how to divide work between threads, factors that affect performance and how data access patterns to data structures can affect concurrency.

We also need to consider things such as exception safety and scalability. Code is said to be scalable if the performance (whether in terms of reduced speed of execution or throughput) increases as more processing cores are added to the system. Ideally the performance increase is linear.

### Exception Safety in parallel algorithms
Concurrent applications need to exception proof in it’s design, this is because if a function spawned on a new thread exits with an exception, the application is terminated.

Here is a naive implementation of `std::accumulate` in which we will use to address several design concerns related to exceptions:
```c++
template<typename Iterator, typename T>
struct accumulate_block
{
	void operator()(Iterator first, Iterator last, T& result)
	{
		result=std::accumulate(first, last, result);
	}
}
template<typename Iterator, typename T>
T parallel_accumulate(Iterator first, Iterator Last, T init)
{
	unsigned long const length=std::distance(first last);
	if(!length)
		return init;
	unsigned long const min_per_thread=25;
	unsigned long const max_threads=
		(length+min_per_thread-1)/min_per_thread;
	unsigned long const hardware_threads=
		std::thread::hardware_concurrency();
	unsigned long const num_threads=
		std::min(hardware_threads!=0?hardware_threads:2, max_threads);
	unsigned long const block_size=length/num_threads;
	std::vector<T> results(num_threads);
	std::vector<std::thread> threads(num_threads-1);
	Iterator block_start=first;
	for(unisgned long i=0; i<(num_thread-1); ++i)
	{
		Iterator block_end=block_start;
		std::advance(block_end, block_size);
		thread[i]=std::thread(
			accumulate_block<Iterator, T>(),
			block_start, block_end, std::ref(results[i])
		);
		block_start=block_end;
	}
	accumulate_block<Iterator, T>()(
		block_start, last, results[num_threads-1];
	std::for_each(
		threads.begin(), threads.end(), std::mem_fn(&std::thread::join));
	return std::accumulate(results.begin(), results.end(), init);
}
```
I will now list places in this example where exceptions can be thrown:
1) the call to `distance` could throw, but because no work that spawns a a new thread has been done plus the invocation is on the calling thread, it’s mostly fine.
2) Next is the allocation of the `results` and `threads` vector, again no actual work has been done(No threads spawned) here plus it’s on the calling threads, it’s mostly fine.
3) `block_start` is also fine. (No work done yet)
4) In the thread spawning loop, once the first thread has been created/spawned we are now at risk of an exception being  thrown triggering the destructor of the thread object to `std::terminate` the program entirely
5) `std::accumulate_block` just after the thread spawning loop can also throw an exception with the similar consequence above-mentioned.
6) the final call to `std:.accumulate` can throw without much issue, since all threads has been joined at before this point. That’s for the main thread.
7) The calls to `accumulate_block` on the new threads might throw at the `result` assignment and since there aren’t any catch blocks, the exception will be un-handled causing the library to call `std::terminate` aborting the application
This code is not exception safe
#### Adding Exception Safety
Here is an update version of the prior example leveraging `std::packaged_task`s and `std::futures` to enforce exception safety:
```c++
template<typename Iterator, typename T>
struct accumulate_block
{
	T operator()(Iterator first, Iterator last)
	{
		return std::accumulate(first, last, T());
	}
};
template<typename Iterator, typename T>
T parallel_accumulate(Iterator first, Iterator last, T init)
{
	unsigned long const length=std::distance(first, last);
	if(!length)
		return init;
	unsigned long const min_per_thread=25;
	unsigned long const max_threads=
		(length+min_per_thread-1)/min_per_thread;
	unsigned long const hardware_threads=
		std::thread::hardware_concurrency();
	unsigned long const block_size=length/num_threads;
	std::vector<std::future<T>> futures(num_threads-1);
	std::vector<std::thread> threads(num_threads-1);
	Iterator block_start=first;
	for(unsigned long i=0; i<(num_threads-1); ++i)
	{
		Iterator block_end=block_start;
		std::advance(block_end, block_size);
		std::packaged_task<T(Iterator, Iterator)> task(
			accumulate_block<Iterator, T>());
		futures[i]=task.get_future();
		threads[i]=std::thread(std::move(task), block_start, block_end);
		block_start=block_end;
	}
	T last_result=accumulate_block<Iterator, T>()(block_start, last);
	std::for_each(threads.begin(), threads.end(),
		std::mem_fn(&std::thread::join));
	T result=init;
	for(unsigned long i=0; i<(num_threads-1); ++i)
		results+=futures[i].get();
	result += last_result;
	return result;
}
```
The list outlines the changes made, and how beneficial they role in making the code exception safe:
1) `accumulate_block` now returns result directly instead of storing them in some referenced variable
2) Since we are now rely on `std::packaged_task`  and `std::futures` to transfer results, any exception thrown will be stored in the future, to be handled by the user later.
3) the resultant of all asynchronous task is then retrieved via a basic loop that goes through the stored futures and invokes their `.get()`, here any captured exception is then re-thrown to be handled.
4) Lastly the result from the last task block is added before returning the final resultant value back to the caller.
All this removed one the potential problems faces before; exceptions thrown in worker threads are now re-thrown in the main thread. It should be noted that if more than one thread throws an exception only one will be propagated, if that is too big a deal and it matters to you for some reason one can use an `std::nested_exception` to capture all exceptions and throw that instead.

The remaining problem now is the blindspot with leaking threads if an exception is thrown between when the first threads is spawned and when we’ve joined them all:  The simplest solution is to catch the exception, join `.joinable()` threads and then rethrow the exception:
```c++
try
{
	for(unsigned long i=0; i<(num_threads-1); ++i)
	{
		// ..just as before
	}
	T last result = accumulate_block<Iterator, T>()(block_start, last);
	std::for_each(threads.begin(), threads.end(),
		std::mem_fn(&std::thread::join));
}
catch(...)
{
	for(unsigned long i=0; i<(num_thread-1); ++i)
	{
		if(threads[i].joinable())
			threads[i].join();
	}
	throw;
}
```
This works, but quite ugly plus it’s not DRY. A better ways its to extract this out into the destructor of an object; after all it’s the idiomatic way of cleaning up resources in C++:
```c++
class join_threads
{
	std::vector<std::thread>& threads;
public:
	explicit join_threads(std::vector<std::thread>& threads_):
		threads(threads_)
	{}
	~join_threads()
	{
		for(unsigned long i=0; i<threads.size(); ++i)
		{
			if(threads[i].joinable())
				threads[i].join();
		}
	}
}
```
This way, no matter how the calling thread exits (due to an exception or whatever), the destructor will be invoked and all the specified threads must be joined before exit.
