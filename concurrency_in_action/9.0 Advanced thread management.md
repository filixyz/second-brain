This chapter discusses advanced thread management like thread pools and such.
## Thread Pools
A thread pool is a mechanism composed of a set of threads and a task supplying queue, each task in the queue is assigned an available worker thread from the set, for said thread to return back to the pool after handling it’s task; awaiting newer task to handle.

There are several key design issues when building a thread pool, such as:
1) How many thread to use
2) The most efficient way to allocate tasks to threads and,
3) Whether or not one can wait for a task to complete.
### The simplest possible thread pool
At simplest a thread pool is a fixed number of threads (typically the same as `std::thread::hardware_concurrency()`) that process work. Each worker thread tasks work off the queue, executes the task and then goes back to the queue for more work. In the simplest case there’s no way to wait for the task to complete, an implementation is listed below:
```c++
class thread_pool
{
	std::atomic<bool> done;
	threadsafe_queue<std::function<void()>> work_queue;
	std::vector<std::threads> threads;
	join_threads joiner;
	void worker_thread()
	{
		while(!done)
		{
			std::function<void()> task;
			if(work_queue.try_pop(task))
				task();
			else
				std::this_thread::yield();
		}
	}
	public:
		thread_pool(): done(false), joiner(threads)
		{
			unsigned const thread_count=
				std::thread::hardware_concurrency();
			try
			{
				for(unsigned i=0; i<thread_count; ++i)
					threads.push_back(
						std::thread(&thread_pool::worker_thread, this));
			}
			catch(...)
			{
				done=true;
				throw;
			}
		}
		~thread_pool()
		{
			done=true;
		}
		template<typename FunctionType>
		void submit(FunctionType f)
		{
			work_queue.push(std::function<void()>(f));
		}
};
```
Something to note is that the order of the data member declarations are important to ensure that objects are destroyed in the right order. For example you can’t destroy the queue safely until all the threads have stopped (since `joiner`s destructor will be invoked first)

For many purposes this simple thread pool with suffice, especially if the tasks are independent and don’t return any values or perform any blocking operation. But there exist many cases where this implementation will not suffice, sometimes even causing deadlocks. The next subsection will address how to implement thread pools that enables one to wait for task submitted

This simple thread pool can be broken down simply into 4 parts:
1) a queue that accepts new task to be executed
2) A vector that manages/contains the threads the pool can provide, with each of them running the same mechanism
3) The mechanism being a simple infinitely looping function that actively prompts the task queue for a new task to execute or yields to relinquish its executive privileges to the OS if its request was unsuccessful
4) a boolean flag that terminates all executing threads in the pool. it is invoked by the destructor.
### Thread pools that handle tasks that can be waited for/on
```c++
class function_wrapper
{
	struct impl_base
	{
		virtual void call() =0;
		virtual ~impl_base() {}
	}
	std::unique_ptr<impl_base> impl;
	template <typename F>
	struct impl_type: impl_base
	{
		F f;
		impl_type(F&& f_): f(std::move(f_)) {}
		void call() { f(); }
	}
public:
	template<typename F>
	function_wrapper(F&& f): impl(new impl_type<F>(std::move(f))) {}
	function_wrapper() = default;
	void operator()() { impl->call(); }
	function_wrapper(function_wrapper&& other): 
		impl(std::move(other.impl))
	{}
	function_wrapper& operator=(function_wrapper&& other)
	{
		impl=std::move(other.impl);
		return *this;
	}
	function_wrapper(const function_wrapper&)=delete;
	function_wrapper& operator=(const function_wrapper&)=delete;
	function_wrapper(function_wrapper&)=delete;
};

class thread_pool
{
	thread_safe_queue<function_wrapper> task_queue;
	void worker_thread()
	{
		while(!done)
		{
			function_wrapper task;
			if(task_queue.try_pop(task))
				task();
			else
				std::this_thread::yield();
		}
	}
public:
	template <typename FunctionType>
	std::future<typename std::result_of<FunctionType()>::type>
		submit(FunctionType f)
	{
		typedef typename std::result_of<FunctionType()>::type result_type;
		std::packaged_task<result_type()> task(std::move(f));
		std::future res(task.get_future());
		task_queue.push(std::move(task));
		return res;
	}
	// rest as before
};
```
This implementation can be useful in situations where the calling thread would like to wait for the resultant of its task after submitting it to a thread pool.
This pool allows us to wait for tasks and have them return results.

Below is an example that shows what the `parallel_accumulate` function looks like with this thread pool:
```c++
template<typename Iterator, typename T>
T parallel_accumulate(Iterator first, Iterator last, T init)
{
	unsigned long const length=std::distance(first, last);
	if(!length)
		return init;
	unsigned long const block_size=25;
	unsigned long const num_blocks=(length+block_size-1)/block_size;
	std::vector<std::future<T>> futures(num_blocks-1);
	thread_pool pool;
	Iterator block_start=first;
	for(unsigned i=0; i<(num_blocks-1); ++i)
	{
		Iterator block_end=first;
		std::advance(block_end, block_size);
		futures[i]=pool.submit([=]{
			accumulate_block<Iterator, T>(block_start, block_end);
		})
		block_start=block_end;
	}
	T last_result=accumulate_block<Iterator, T>(block_start, last);
	T result=init;
	for(unsigned i=0; i<(num_blocks-1); ++i)
		result+=futures[i].get();
	result += last_result;
	return result;
}
```

This type of thread pool works well for cases in which the tasks are independent but it’s not so great for situations where tasks depend on other tasks also submitted to the thread pool

###  Thread Pools that handle tasks that wait for other tasks
To illustrate this concept the quicksort algorithm will be used. Chapter 8 uses an alternative method in approaching `quicksort` in which the data to be sorted is divided, at every layer of division, into two chunks; one to be executed recursively and the other is placed in a stack, or in this case thread pool, for chunks to be sorted asynchronously.

Relying on our current thread pool implementation for this mechanism quickly becomes inefficient because; since thread pools are collection of threads of some limited size, the quicksorter might finds itself in a situation, at some point, where all threads in it’s thread pool are all waiting on some chunk that has not be sorted and those chunks being waited on can never be sorted because the worker threads in said thread pool have been exhausted, since they all are occupied with waiting. The quicksorter and its thread pool are now in a deadlock.

A solution to this issue would be define a new function in the thread pool that the quick sorter can invoke when it finds itself in a situation where it’s currently waiting for some unsorted chunk; so instead of waiting it uses the resources it would have wasted doing that to sort an unsorted chunk (possibly even the chunk that it was waiting for).

This new function directive is simply to enable the manager of the thread pool run task from the queue and manage the loop themselve.
```c++
void thread_pool::run_pending_task()
{
	function_wrapper task;
	if(task_queue.try_pop(task))
		task;
	else
		std::this_thread::yield();
}
```

Below is a thread pool based implementation of quick sort:
```c++
template<typename T>
struct sorter
{
	thread_pool pool;
	std::list<T> do_sort(std::list<T>& chunk_data)
	{
		if (chunk_data.is_empty())
			return chunk_data;
		std::list<T> result;
		result.splice(result.begin(), chunk_data, chunk_data.begin());
		T const& partition_div=*result.begin();
		typename std::list<T>::iterator divide_point=
			std::partition(chunk_data.begin(), chunk_data.end(), 
				[&](T const val){ return val<partition_div; });
		std::list<T> new_lower;
		std::splice(new_lower.end(), chunk_data, chunk_data.begin(),
			divide_point);
		std::future<std::list<T>> lower_res=
			pool.submit(std::bind(&sorter::do_sort, this,
				std::move(new_lower)));
		std::list<T> new_higher{do_sort(chunk_data)};
		while(lower_res.wait_for(std::chrono::seconds(0)) == 
			std::future_status::timeout)
		{
			pool.run_pending_task();
		}
		result.splice(result.end(), new_higher);
		result.splice(result.begin(), lower_res.get());
		return result;
	}
};
```

This implementation, although satisfactory in some cases, is still far from perfect, every call to submit and every call to `run_pending_task()` access the same queue, and we’ve seen in chapter 8 that having a single set of data modified by multiple threads can have a detrimental effect on performance (could cause cache ping pongs) so we need to address the issue.

### Avoiding contention on the work queue
Since the threads that utilize are current thread pool all share the same task queue, meaning all thread that invoke `.submit()` all push task to the same task queue likewise any thread that pushes a task inadvertently  pushes to the same task queue, as the number of processors increases in this scenario, so does the number of possible threads that could access the queue increase meaning the degree of contention in the task queue also increases, which also increases the risk of performance impacts due to cache ping ping and even, in cases where a the queue is lock based, thread serialization.

A solution to this would be for each worker thread to own an individual work queue separate from the global work queue with the aim that the worker threads only query the global task queue when its own individual task queue is empty.

below is the implementation of said solution, leveraging on `thread_local` to enforce that each thread possesses their own individual task queue:
```c++
class thread_pool
{
	thread_safe_queue<function_wrapper> global_task_queue;
	typedef std::queue<function_wrapper> local_task_queue_t;
	static thread_local std::unique_ptr<local_task_queue_t> 
		local_task_queue;
	void worker_thread()
	{
		local_task_queue.reset(new local_task_queue_t);
		while(!done)
			run_pending_task();
	}
public:
	template <typename FunctionType>
	std::future<typename std::result_of<FunctionType()>::type>
		submit(FunctionType f)
	{
		typedef typename std::result_of<FunctionType()>::type 
			result_type;
		std::packaged_task<result_type()> task(f);
		std::future<result_type> res{task.get_future()};
		if(local_task_queue)
			local_task_queue->push(std::move(task));
		else
			global_task_queue.push(std::move(task));
		return res;
	}
	void run_pending_task()
	{
		function_wrapper task;
		if(local_task_queue && !local_task_queue->is_empty())
		{
			task=std::move(local_task_queue->first());
			local_task_queue.pop();
			task();
		} 
		else if(global_task_queue.try_pop(task))
		{
			task();
		} 
		else
			std::this_thread::yield();
	}
	// rest as before
};
```
in this implementation we use `std::unique_ptr` to hold instances of local queues so not to allow threads that aren’t part of the thread pool to own a local queue, since it’s already enforced that only worker threads can have a valid unique pointer that points to an actual local queue due to  statement
```cpp 
local_task_queue.reset(new local_task_queue_t)
```
declared in private function `thread_pool::worker_thread()`, the destructor of `unqiue_ptr` ensures the local queue is destroyed upon exit of the thread.

both `submit` and `run_pending_task` ensure to submit/execute tasks to/from it’s threads’ local queue **if** it’s thread is a worker thread, in the event that there are no task in it’s local queue/ or it’s thread is not a worker thread; they interact with the global queue.

This works fine for reducing contention, but when the distribution of work is uneven, it quickly becomes ineffective spawning situations in which one thread has lots of work to handle while others have none, defeating the purpose of using a thread pool.

An example of this can be seen with the quick-sort we implemented earlier, if our current thread pool was used to handle the threading needs of the algorithm; we would come to notice that only the first/topmost chunk will be sent to the global queue, while the rest of the chunks would end up in the local thread queue of the worker thread that handled that first/topmost chunk.

Thankfully, there exists a solution to this issue… **Work stealing**

### Work Stealing
This is simply enabling worker threads, with no work to do, take work from another thread with a full work queue.

This entails that the thread doing the stealing should be able to access the stolen from queue via `run_pending_task()`, which, in turn, requires that each thread register it’s queue with the thread pool (global queue).

Below is the implementation of a lock based queue that allows for work stealing, we hope that work stealing is a rare event so there should be little contention on the mutex of the stolen from queue:
```c++
class work_stealing_queue
{
private:
	typedef function_wrapper data_type;
	std::deque<data_type> the_queue;
	mutable std::mutex the_mutex;
public:
	work_stealing_queue() {}
	work_stealing_queue(const work_stealing_queue&)=delete;
	work_stealing_queue& operator=(const work_stealing_queue&)=delete;
	
	void push(data_type data)
	{
		std::lock_guard<std::mutex> lock {the_mutex};
		the_queue.push_front(std::move(data));
	}
	void empty() const
	{
		std::lock_guard<std::mutex> lock {the_mutex};
		return the_queue.empty();
	}
	bool try_pop(data_type& data)
	{
		std::lock_guard<std::mutex> lock{the_mutex};
		if(the_queue.empty())
			return false
		data=std::move(the_queue.front());
		the_queue.pop_front();
		return true;
	}
	bool try_steal(data_type& res)
	{
		std::lock_guard<std::mutex> lock{the_mutex};
		if(the_queue.empty())
			return false;
		res=std::move(the_queue.back());
		the_queue.pop_back();
		return false;
	}
};
```
This “queue” is a LIFO container for its own thread while the opposite’s the case for work stealing threads. 

Since the most recent task are the ones being handled by worker threads, This can prove to be beneficial from the caches perspective since the data related to the task is more likely to still be in the cache than the data related to the task pushed on the queue previously.

And given that `try_steal` feeds off tasks at the back of the stolen form “queue”,  This can minimize contention on the queue.

Below is an implementation of a thread pool that uses work stealing:
```c++
class thread_pool
{
	std::atomic<bool> done;
	thread_safe_queue<function_wrapper> global_queue;
	std::vector<std::unique_ptr<work_stealing_queue>> worker_queues;
	std::vector<std::threads> worker_threads;
	join_threads joiner;
	static thread_local work_stealing_queue* local_queue;
	static thread_local unsigned thread_index;
	
	void worker_threads(unsigned const index)
	{
		thread_index=index;
		local_queue=worker_queue[thread_index].get();
		while(!done)
			run_pending_task();
	}
	bool try_to_get_task_from_global_queue(function_wrapper& task)
	{
		return global_queue.try_pop(task);
	}
	bool try_to_get_task_from_local_queue(function_wrapper& task)
	{
		return local_queue && local_queue->try_pop(task);
	}
	bool try_and_steal_from_another_worker_queue(function_wrapper& task)
	{
		for(unsigned i=0; i<worker_queue.size(); ++i)
		{
			unsigned index=(thread_index+i+1)%worker_queue.size();
			if(worker_queues[index]->try_steal(task))
				return true;
		}
		return false;
	}
public:
	thread_pool(): done(false), joiner(worker_threads)
	{
		unsigned const thread_count=std::thread::hardware_concurrency();
		try
		{
			worker_queues=std::vector<std::unique_ptr<
				work_stealing_queue>> (thread_count);
			for(unsigned i=0; i<thread_count; ++i)
			{
				worker_queues.reset(new work_stealing_queue);
			}
			for(unsigned i=0; i<thread_count; ++i)
			{
				worker_threads.push_back(std::thread(
					&thread_pool::worker_thread, this, i));
			}
		} catch(...)
		{
			done=true;
			throw;
		}
	}
	// other contructors go here <!!SHOULD BE NON-COPYABLE!!>
	~thread_pool()
	{
		done=true;
	}
	template <typename FunctionType>
	std::future<typename std::result_of<FunctionType()>::type>
		submit(FunctionType func)
	{
		typedef typename std::result_of<FunctionType()>::type> 
			result_type;
		std::packaged_task<result_type()> task {std::move(func)};
		std::future<result_type> res=task.get_future();
		if(local_queue)
			local_queue->push(std::move(task));
		else
			global_queue.push(std::move(task));
		return res;
	}
	void run_pending_task()
	{
		function_wrapper task;
		if(try_to_get_from_local_queue(task) ||
		   try_to_get_from_global_queue(task) ||
		   try_to_steal_from_another_worker_queue(task))
		{
			task();
		} 
		else
			std::this_thread::yield();
	}
};
```
refer to task for further explanation.

## Interrupting Threads
in many situation sits is desirable to signal a running thread to terminate itself, this might be that the thread is a worker thread for a thread pool and that pool is being destroyed or the thread has been explicitly cancelled by the user.

Whatever the reason it might be the idea for handling this is the same; the thread doing the interrupting needs a mechanism to signal its victim thread to terminate nicely instead of pulling the rug over it abruptly.

### Launching and interrupting another thread
First of let’s look at the external interface, what do we need for an `interruptible_thread`? Answer; The handle of generic thread with an added `interrupt()` handle:
```c++
class interruptible_thread
{
public:
	template<typename FunctionType>
	interruptible_thread(FunctionType& func);
	void detach();
	void join();
	bool joinable() const;
	void interrupt();
};
```
Internally one can use an `std::thread` instance to manage the thread to be launched, but what of in the perspective of the thread being interrupted itself?, we need a signal or flag said thread can evaluate so it knows that its should terminate.

This is achieved by introducing a `thread_local` atomic boolean flag that an interruptible thread can rely on to know when it has been signaled to terminate. An implementation of an interruptible thread incorporating is defined below:
```c++
class interrupt_flag
{
public:
	void set();
	bool is_set() bool;
};
thread_local interrupt_flag this_thread_interrupt_flag;
class interruptible_thread()
{
	std::thread internal_thread;
	interrupt_flag* internal_flag;
public:
	template<typename FunctionType>
	interrupt_thread(FunctionType f)
	{
		std::promise<interrupt_flag*> p;
		internal_thread=std::thread([f, &p]{
			p.set_value(&this_thread_interrupt_flag);
			f();
		});
		internal_flag=&p.get_future().get();
	}
	
	void interrupt()
	{
		if(flag)
			internal_flag->set();
	}
};
```
In the `interrupt_thread()` constructor, the lambda takes a copy of `f` and references only `p` so any other referencing would be done with what is already present in the newly spawned thread, this allows the `this_thread_interrupt_flag` referenced in the newly spawned thread body, evaluate to the actual interrupt flag local to that thread.

### Detecting that a thread has been interrupted
Now we need a mechanism that allows the thread being interrupted to check if it’s interrupt flag has been set so it can proceed to cleanly terminate itself.

The simplest way to handle this would be with a function that the thread to be interrupted can place within itself wherever it deems safe to allow termination to occur, an `interruption_point()` function.

This functions directive would be check if the threads local interrupt flag has been set and of this is proven true an exception is thrown to terminate the thread.

(the function can also be extended to perform explicit cleanups that might be needed for safe termination). Below is it’s implementation:
```c++
void interruption_point()
{
	if(this_thread_interrupt_flag.is_set())
		throw thread_interrupted();
}
```
We can use this function by invoking it at convenient points in the threads code:
```c++
void foo()
{
	while(!done)
	{
		interruption_point();  // can possibly interrupt here
		do_further_processing();
	}
}
```
The main aim of `interruption_point()` is to give interruptible threads in execution a way of saying “hey, if i am to terminate, i would prefer terminating here.”

### Interrupting a condition variable wait
With `interruption_point()` we can declare spots in our threads we would prefer terminating at, but what of cases where our executing thread is waiting for some thing, like a condition variable, and we would like to interrupt it even while it’s in a waiting state, how could we achieve this?

We could achieve this associating the interrupt flag of the thread to the condition variable that thread is currently waiting for, so when another thread chooses to interrupt, the interrupt flag is set and the associated conditional variable is notified to awake the thread if in a waiting state. Once the thread is woken and the conditional variable `.wait()` returns an `interruption_point()`  is placed placed right after so termination can take place.

The text encapsulates this mechanism into the function `interruptible_wait()`:
```c++
void interruptible_wait(std::conditional_variable& cv,
	std::unique_lock<std::mutex>& lk)
{
	interruption_point();
	this_thread_interrupt_flag.set_condition_variable(cv);
	cv.wait(lk);
	this_thread_interrupt_flag.clear_condition_variable(cv);
	interruption_point();
}
```
This implementation seems perfect at first glance but it imposes lots of problems:
1) Firstly an exception can be thrown by `cv.wait(lk)` and if it happens the conditional variable associated to this threads interrupt flag cannot be broken anymore. This is easily fixed by declaring a wrapper that breaks this association in it’s destructor so however the threads chooses to terminate, that association is guaranteed to be broken.
2) A race condition exists in-between the first call to `interruption_point` and just right before the execution of `cv.wait(lk)`; in the scenario that an interruption is flagged within this space frame, the condition variable’s `wait()` can no longer be interrupted by it’s interrupt flags’ .`set()`, this is because the interruption was lodged prior to when the condition variable began waiting making it impossible for it to see previous interruption related notifications (eg. a`notify_one()` or `notify_all()` by `interrupt_flag::set()`), the only time the thread will awake to be interrupted will be when the condition it’s waiting for has been met, which defeats the whole purpose of being able to interrupt even while waiting.

A solution to problem 2 would be to impose a time limit on the condition variable’s wait with `std::condition_variable.wait_for()`. This is done to make the conditional wait time bounded forcing the thread to awake so  `interruption_point()` can be invoked.

This time bound means the thread can experience multiple spurious wakes but this cannot be helped.

The time limit the text was 1 millisecond. Below is an implementation that incorporates all discussed in this subsection
```c++
class interrupt_flag
{
	std::atomic<bool> internal_flag
	std::condition_variable* condition;
	std::mutex set_clear_mutex;
public:
	interrupt_flag():
		condition(0)
	{}
	void set()
	{
		internal_flag.store(true, std::memory_order_relaxed);
		std::lock_guard<std::mutex> lk{set_clear_mutex};
		if(condition)
			condition->notify_all();
	}
	void set_condition_variable(std::condition_variable& cv)
	{
		std::lock_guard<std::mutex> lk {set_clear_mutex};
		condition=&cv;
	}
	void clear_condition_variable()
	{
		std::lock_guard<std::mutex> lk (set_clear_mutex);
		condition=0;
	}
	bool is_set() const
	{
		return internal_flag.load(std::memory_order_relaxed);
	}
	struct remove_cv_on_destruct()
	{
		~remove_cv_on_destruct(){
			this_thread_interrupt_flag.clear_condition_variable();
		}
	};
};

void interruptible_wait(std::condition_variable& cv,
	std::unqiue_lock<std::mutex>& lk)
{
	interruption_point();
	this_thread_interrupt_flag.set_condition_variable(cv);
	interrupt_flag::remove_cv_on_destruct guard();
	interruption_point();
	cv.wait_for(lk, std::chrono::milliseconds(1));
	interruption_point();
}
```
in the case the thread is waiting for some predicate, then the timeout can be completely hidden inside the predicate loop:
```c++
template<typename Predicate>
void interruptible_wait(std::condition_variable& cv,
					   std::unique_lock<std::mutex>& lk
					   Predicate& pred)
{
	interruption_point();
	this_thread_interrupt_flag.set_condition_variable(cv);
	interrupt_flag::remove_cv_on_destruct guard();
	do
		cv.wait_for(lk, std::chrono::milliseconds(1));
	while(!this_thread_interrupt_flag.is_set() && !pred())
	interruption_point();
}
```

### Interrupting a wait on `std::condition_variable_any`
`std::condition_variable_any` differs from it’s normative variant in that it works with any lock type. 

This makes things easier since we can create our own custom lock that encapsulates locking both `set_clear_mutex` and the mutex of the conditional variable the interruptible thread could be waiting on.  

Doing this completely dissolves the issue of a race condition being apparent when a notification is sent to a thread to interrupt but said thread condition variable has not begun waiting for any form of notification.

Our custom lock would 
1) upon construction acquire it’s thread interrupts flag mutex then proceed to immediately associate the threads condition variable to the interrupt flag (all done during construction).
2) ensure it’s `.lock()` acquires both the interrupts flag `set_clear_mutex` and the conditional variables mutex simultaneously
3) ensure it’s `.unlock()`relinquishes both locks at the same time
4) when destructing, unlock the threads’ interrupts flag `set_clear_mutex` after ensuring the it’s condition variable has been disassociated

This in turn would implicitly enforce:
-  that an interruption notification broadcast cannot happen when or before the interruptible thread associates it’s condition variable to it’s interrupt flag, only after.
- that any interrupt notification broadcast is serialized only to occur when the conditional variable is actively waiting; ready to receive any sort of notification (either interrupt related or condition related). 

Since while waiting both locks will be free which allows serialized threads to acquire `set_clear_mutex` and publish an interrupt notification (via `notify_all()`) that the conditional variable should notice.

and while awake and unable to perceive notifications both locks should be acquired making it impossible for other threads to send interrupt notification broadcasts that will ultimately end up unnoticed.

Below shows an implementation of an interrupt flag that incorporates the custom lock we discussed in a wait member function. This `wait()` allows us to wait for some condition in an interruptible thread by directly referencing its interrupt flag so all we just discussed can be implicitly enforced:
```c++
void interrupt_flag
{
	std::atomic<bool> internal_flag;
	std::condition_variable* thread_cond;
	std::condition_variable_any* thread_cond_any;
	std::mutex set_clear_mutex;
public:
	interrupt_flag(): thread_cond(0), thread_cond_any(0)
	{}
	void set()
	{
		internal_flag.store(true, std::memory_order_relaxed);
		std::lock_guard<std::mutex> lk;
		if(thread_cond)
			thread_cond->notify_all();
		if(thread_cond_any)
			thread_cond_any->notify_all();
	}
	template<typename Lockable>
	void wait(std::conditional_variable& cond, Lockable& lk)
	{
		struct custom_lock
		{
			internal_flag* self;
			Lockable& lock;
			custom_lock(internal_flag* self_, Lockable& lock_,
				std::condition_variable_any& cond)
				: self(self_), lock(lock)
			{
				self->set_clear_mutex.lock();
				self->thread_cond_any=&cond;
			}
			void lock()
			{
				std::lock(self->set_clear_mutex, lock);
			}
			void unlock()
			{
				lock.unlock();
				self->set_clear_mutex.unlock();
			}
			void ~custom_lock()
			{
				self->thread_cond_any=0;
				self->set_clear_mutex.unlock;
			}
		};
		custom_lock cl (this, cond, lk);
		interruption_point();
		cond.wait(cl);
		interruption_point();
	}
	// rest as before
};

template<typename Lockable>
void interruptible_wait(std::condition_variable_any& cva, Lockable& lk)
{
	this_thread_interrupt_flag.wait(cva, lk);
}
```

### Interrupting other blocking calls
This rounds up waits on conditional variables, but what about waiting on other blocking calls like, waiting on mutexes or futures and such? 

In short we would have to go for the timeout option, since there is no way to interrupt a wait short of fulfilling the condition being waited for without access to the internals of the mutex of future.

But since we know what we are waiting for we can loop within the interruptible wait to to terminate the thread once certain that the interrupt flag has been set and the *thing* being waited for has not concluded.

Below is an overload of `interruptible_wait()` for `std::future<>`:
```c++
template<typename T>
void interruptible_wait(std::future<T>& uf)
{
	while(!this_thread_interruption_flag.is_set())
	{
		if(uf.wait_for(std::chrono::milliseconds(1)) == 
			std::future_status::ready)
			break;
	}
	interruption_point();
}
```
the text wrongfully places an `lk` argument in `uf.wait_for()`

### Handling Interruptions
From the point of view of the thread being interrupted, an interruption is a `thread_interrupted exception`, which can be handled like any other exception , for example:
```c++
try
{
	do_something();
}
catch(thread_interruption&)
{
	handle_interruption();
}
```
This allows us the thread to perceive that it has been instructed to interrupt, possibly conclude what it was currently executing and carry on regardless. This can be useful when a thread has a list of independent tasks to handle and was instructed (for some reason) to interrupt while handling one of them, it can then proceed to discard that task and start handling a new one, possibly to be interrupted again next time it calls an interruption point.

-------------------------
We should not allow an interrupted threads interrupt exception propagate out of the **actual** function the interruptible threads’ instance was declared to execute (not the internally wrapped function), This is because underneath, an interruptible thread is still an `std::thread` instance so in the situation that this happens `std::thread` is designed to invoke `std::terminate` to terminate the program as a whole.

To avoid this in our `interruptible_thread`, a solution would be:
* when defining the `internal_thread` in our constructor, a try-catch block should be set in the body of the internal threads’ **actual** function, so in the event a `thread_interruption` exception propagates out of the wrapped internal function that exception can be intercepted.

Doing this, the initialization of the thread in `interruptible_thread` constructor should now look like this:
```c++
internal_thread=std::thread([f, &p]{  // actual functioon
	p.set_value(&this_thread_intercept_flag);
	try
	{
		f();  // wrapped internal function
	}
	catch(thread_interrupted const&)
	{}
});
```

### Interrupting background tasks on application exit
Below is a file search program that monitors the file-system in the background in order to keep it’s indexes updated. This example illustrates how we could rely on interruptible threads to interrupt background tasks on application exit.
Refer to the text if confused later:
```c++
std::mutex config_mutex;
std::vector<interruptible_thread> background_threads;

void background_thread(int disk_id)
{
	while(true)
	{
		interruption_point();
		fs_change fsc=get_fs_changes(disk_id);
		if(fsc.has_changes())
			update_index(fsc);
	}
}
void start_background_processing()
{
	background_threads.push_back(
		interruptible_thread(background_thread, disk_1));
	background_threads.push_back(
		interruptible_thread(background_thread, disk_2));
}
int main()
{
	start_background_processing();
	process_gui_until_exit();
	std::unique_lock<std::mutex> lk(config_mutex);
	for(unsigned i=0; i<background_threads.size(); ++i)
		background_threads[i].interrupt();
	for(unsigned i=0; i<background_threads.size(); ++i)
		background_threads[i].join();
}
```