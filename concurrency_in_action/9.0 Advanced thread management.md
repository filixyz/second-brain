This chapter discusses advanced thread management like thread pools and such.
## Thread Pools
A thread pool is a mechanism composed of a set of threads and a task supplying queue, each task in the queue is assigned an available worker thread from the set, for said thread to return back to the pool after handling it’s task; awaiting newer task to handle.

There are several key design issues when building a thread pool, such as:
1) How many thread to use
2) The most efficient way to allocate tasks to threads and,
3) Whether or not one can wait for a task to complete.
### The simplest possible thread pool
At simplest a thread pool is a fixed number of threads (typically the same as `std::thread::hardware_concurrency()`) that process work. Each worker thread tasks work off the queue, executes the task and then goes back to the queue for more work. In the simplest case there’s no way to wait for the task to complete, an implementation is listed below:
```c++
class thread_pool
{
	std::atomic<bool> done;
	threadsafe_queue<std::function<void()>> work_queue;
	std::vector<std::threads> threads;
	join_threads joiner;
	void worker_thread()
	{
		while(!done)
		{
			std::function<void()> task;
			if(work_queue.try_pop(task))
				task();
			else
				std::this_thread::yield();
		}
	}
	public:
		thread_pool(): done(false), joiner(threads)
		{
			unsigned const thread_count=
				std::thread::hardware_concurrency();
			try
			{
				for(unsigned i=0; i<thread_count; ++i)
					threads.push_back(
						std::thread(&thread_pool::worker_thread, this));
			}
			catch(...)
			{
				done=true;
				throw;
			}
		}
		~thread_pool()
		{
			done=true;
		}
		template<typename FunctionType>
		void submit(FunctionType f)
		{
			work_queue.push(std::function<void()>(f));
		}
};
```
Something to note is that the order of the data member declarations are important to ensure that objects are destroyed in the right order. For example you can’t destroy the queue safely until all the threads have stopped (since `joiner`s destructor will be invoked first)

For many purposes this simple thread pool with suffice, especially if the tasks are independent and don’t return any values or perform any blocking operation. But there exist many cases where this implementation will not suffice, sometimes even causing deadlocks. The next subsection will address how to implement thread pools that enables one to wait for task submitted

This simple thread pool can be broken down simply into 4 parts:
1) a queue that accepts new task to be executed
2) A vector that manages/contains the threads the pool can provide, with each of them running the same mechanism
3) The mechanism being a simple infinitely looping function that actively prompts the task queue for a new task to execute or yields to relinquish its executive privileges to the OS if its request was unsuccessful
4) a boolean flag that terminates all executing threads in the pool. it is invoked by the destructor.
### Thread pools that handle tasks that can be waited for/on
```c++
class function_wrapper
{
	struct impl_base
	{
		virtual void call() =0;
		virtual ~impl_base() {}
	}
	std::unique_ptr<impl_base> impl;
	template <typename F>
	struct impl_type: impl_base
	{
		F f;
		impl_type(F&& f_): f(std::move(f_)) {}
		void call() { f(); }
	}
public:
	template<typename F>
	function_wrapper(F&& f): impl(new impl_type<F>(std::move(f))) {}
	function_wrapper() = default;
	void operator()() { impl->call(); }
	function_wrapper(function_wrapper&& other): 
		impl(std::move(other.impl))
	{}
	function_wrapper& operator=(function_wrapper&& other)
	{
		impl=std::move(other.impl);
		return *this;
	}
	function_wrapper(const function_wrapper&)=delete;
	function_wrapper& operator=(const function_wrapper&)=delete;
	function_wrapper(function_wrapper&)=delete;
};

class thread_pool
{
	thread_safe_queue<function_wrapper> task_queue;
	void worker_thread()
	{
		while(!done)
		{
			function_wrapper task;
			if(task_queue.try_pop(task))
				task();
			else
				std::this_thread::yield();
		}
	}
public:
	template <typename FunctionType>
	std::future<typename std::result_of<FunctionType()>::type>
		submit(FunctionType f)
	{
		typedef typename std::result_of<FunctionType()>::type result_type;
		std::packaged_task<result_type()> task(std::move(f));
		std::future res(task.get_future());
		task_queue.push(std::move(task));
		return res;
	}
	// rest as before
};
```
This implementation can be useful in situations where the calling thread would like to wait for the resultant of its task after submitting it to a thread pool.
This pool allows us to wait for tasks and have them return results.

Below is an example that shows what the `parallel_accumulate` function looks like with this thread pool:
```c++
template<typename Iterator, typename T>
T parallel_accumulate(Iterator first, Iterator last, T init)
{
	unsigned long const length=std::distance(first, last);
	if(!length)
		return init;
	unsigned long const block_size=25;
	unsigned long const num_blocks=(length+block_size-1)/block_size;
	std::vector<std::future<T>> futures(num_blocks-1);
	thread_pool pool;
	Iterator block_start=first;
	for(unsigned i=0; i<(num_blocks-1); ++i)
	{
		Iterator block_end=first;
		std::advance(block_end, block_size);
		futures[i]=pool.submit([=]{
			accumulate_block<Iterator, T>(block_start, block_end);
		})
		block_start=block_end;
	}
	T last_result=accumulate_block<Iterator, T>(block_start, last);
	T result=init;
	for(unsigned i=0; i<(num_blocks-1); ++i)
		result+=futures[i].get();
	result += last_result;
	return result;
}
```

This type of thread pool works well for cases in which the tasks are independent but it’s not so great for situations where tasks depend on other tasks also submitted to the thread pool

###  Thread Pools that handle tasks that wait for other tasks
To illustrate this concept the quicksort algorithm will be used. Chapter 8 uses an alternative method in approaching `quicksort` in which the data to be sorted is divided, at every layer of division, into two chunks; one to be executed recursively and the other is placed in a stack, or in this case thread pool, for chunks to be sorted asynchronously.

Relying on our current thread pool implementation for this mechanism quickly becomes inefficient because; since thread pools are collection of threads of some limited size, the quicksorter might finds itself in a situation, at some point, where all threads in it’s thread pool are all waiting on some chunk that has not be sorted and those chunks being waited on can never be sorted because the worker threads in said thread pool have been exhausted, since they all are occupied with waiting. The quicksorter and its thread pool are now in a deadlock.

A solution to this issue would be define a new function in the thread pool that the quick sorter can invoke when it finds itself in a situation where it’s currently waiting for some unsorted chunk; so instead of waiting it uses the resources it would have wasted doing that to sort an unsorted chunk (possibly even the chunk that it was waiting for).

This new function directive is simply to enable the manager of the thread pool run task from the queue and manage the loop themselve.
```c++
void thread_pool::run_pending_task()
{
	function_wrapper task;
	if(task_queue.try_pop(task))
		task;
	else
		std::this_thread::yield();
}
```

Below is a thread pool based implementation of quick sort:
```c++
template<typename T>
struct sorter
{
	thread_pool pool;
	std::list<T> do_sort(std::list<T>& chunk_data)
	{
		if (chunk_data.is_empty())
			return chunk_data;
		std::list<T> result;
		result.splice(result.begin(), chunk_data, chunk_data.begin());
		T const& partition_div=*result.begin();
		typename std::list<T>::iterator divide_point=
			std::partition(chunk_data.begin(), chunk_data.end(), 
				[&](T const val){ return val<partition_div; });
		std::list<T> new_lower;
		std::splice(new_lower.end(), chunk_data, chunk_data.begin(),
			divide_point);
		std::future<std::list<T>> lower_res=
			pool.submit(std::bind(&sorter::do_sort, this,
				std::move(new_lower)));
		std::list<T> new_higher{do_sort(chunk_data)};
		while(lower_res.wait_for(std::chrono::seconds(0)) == 
			std::future_status::timeout)
		{
			pool.run_pending_task();
		}
		result.splice(result.end(), new_higher);
		result.splice(result.begin(), lower_res.get());
		return result;
	}
};
```

This implementation, although satisfactory in some cases, is still far from perfect, every call to submit and every call to `run_pending_task()` access the same queue, and we’ve seen in chapter 8 that having a single set of data modified by multiple threads can have a detrimental effect on performance (could cause cache ping pongs) so we need to address the issue.

### Avoiding contention on the work queue
Since the threads that utilize are current thread pool all share the same task queue, meaning all thread that invoke `.submit()` all push task to the same task queue likewise any thread that pushes a task inadvertently  pushes to the same task queue, as the number of processors increases in this scenario, so does the number of possible threads that could access the queue increase meaning the degree of contention in the task queue also increases, which also increases the risk of performance impacts due to cache ping ping and even, in cases where a the queue is lock based, thread serialization.

A solution to this would be for each worker thread to own an individual work queue separate from the global work queue with the aim that the worker threads only query the global task queue when its own individual task queue is empty.

below is the implementation of said solution, leveraging on `thread_local` to enforce that each thread possesses their own individual task queue:
```c++
class thread_pool
{
	thread_safe_queue<function_wrapper> global_task_queue;
	typedef std::queue<function_wrapper> local_task_queue_t;
	static thread_local std::unique_ptr<local_task_queue_t> 
		local_task_queue;
	void worker_thread()
	{
		local_task_queue.reset(new local_task_queue_t);
		while(!done)
			run_pending_task();
	}
public:
	template <typename FunctionType>
	std::future<typename std::result_of<FunctionType()>::type>
		submit(FunctionType f)
	{
		typedef typename std::result_of<FunctionType()>::type 
			result_type;
		std::packaged_task<result_type()> task(f);
		std::future<result_type> res{task.get_future()};
		if(local_task_queue)
			local_task_queue->push(std::move(task));
		else
			global_task_queue.push(std::move(task));
		return res;
	}
	void run_pending_task()
	{
		function_wrapper task;
		if(local_task_queue && !local_task_queue->is_empty())
		{
			task=std::move(local_task_queue->first());
			local_task_queue.pop();
			task();
		} 
		else if(global_task_queue.try_pop(task))
		{
			task();
		} 
		else
			std::this_thread::yield();
	}
	// rest as before
};
```
in this implementation we use `std::unique_ptr` to hold instances of local queues so not to allow threads that aren’t part of the thread pool to own a local queue, since it’s already enforced that only worker threads can have a valid unique pointer that points to an actual local queue due to  statement
```cpp 
local_task_queue.reset(new local_task_queue_t)
```
declared in private function `thread_pool::worker_thread()`, the destructor of `unqiue_ptr` ensures the local queue is destroyed upon exit of the thread.

both `submit` and `run_pending_task` ensure to submit/execute tasks to/from it’s threads’ local queue **if** it’s thread is a worker thread, in the event that there are no task in it’s local queue/ or it’s thread is not a worker thread; they interact with the global queue.

This works fine for reducing contention, but when the distribution of work is uneven, it quickly becomes ineffective spawning situations in which one thread has lots of work to handle while others have none, defeating the purpose of using a thread pool.

An example of this can be seen with the quick-sort we implemented earlier, if our current thread pool was used to handle the threading needs of the algorithm; we would come to notice that only the first/topmost chunk will be sent to the global queue, while the rest of the chunks would end up in the local thread queue of the worker thread that handled that first/topmost chunk.

Thankfully, there exists a solution to this issue… **Work stealing**

### Work Stealing
This is simply enabling worker threads, with no work to do, take work from another thread with a full work queue.

This entails that the thread doing the stealing should be able to access the stolen from queue via `run_pending_task()`, which, in turn, requires that each thread register it’s queue with the thread pool (global queue).

Below is the implementation of a lock based queue that allows for work stealing, we hope that work stealing is a rare event so there should be little contention on the mutex of the stolen from queue:
```c++
class work_stealing_queue
{
private:
	typedef function_wrapper data_type;
	std::deque<data_type> the_queue;
	mutable std::mutex the_mutex;
public:
	work_stealing_queue() {}
	work_stealing_queue(const work_stealing_queue&)=delete;
	work_stealing_queue& operator=(const work_stealing_queue&)=delete;
	
	void push(data_type data)
	{
		std::lock_guard<std::mutex> lock {the_mutex};
		the_queue.push_front(std::move(data));
	}
	void empty() const
	{
		std::lock_guard<std::mutex> lock {the_mutex};
		return the_queue.empty();
	}
	bool try_pop(data_type& data)
	{
		std::lock_guard<std::mutex> lock{the_mutex};
		if(the_queue.empty())
			return false
		data=std::move(the_queue.front());
		the_queue.pop_front();
		return true;
	}
	bool try_steal(data_type& res)
	{
		std::lock_guard<std::mutex> lock{the_mutex};
		if(the_queue.empty())
			return false;
		res=std::move(the_queue.back());
		the_queue.pop_back();
		return false;
	}
};
```
This “queue” is a LIFO container for its own thread while the opposite’s the case for work stealing threads. 

Since the most recent task are the ones being handled by worker threads, This can prove to be beneficial from the caches perspective since the data related to the task is more likely to still be in the cache than the data related to the task pushed on the queue previously.

And given that `try_steal` feeds off tasks at the back of the stolen form “queue”,  This can minimize contention on the queue.

Below is an implementation of a thread pool that uses work stealing:
```c++
class thread_pool
{
	std::atomic<bool> done;
	thread_safe_queue<function_wrapper> global_queue;
	std::vector<std::unique_ptr<work_stealing_queue>> worker_queues;
	std::vector<std::threads> worker_threads;
	join_threads joiner;
	static thread_local work_stealing_queue* local_queue;
	static thread_local unsigned thread_index;
	
	void worker_threads(unsigned const index)
	{
		thread_index=index;
		local_queue=worker_queue[thread_index].get();
		while(!done)
			run_pending_task();
	}
	bool try_to_get_task_from_global_queue(function_wrapper& task)
	{
		return global_queue.try_pop(task);
	}
	bool try_to_get_task_from_local_queue(function_wrapper& task)
	{
		return local_queue && local_queue->try_pop(task);
	}
	bool try_and_steal_from_another_worker_queue(function_wrapper& task)
	{
		for(unsigned i=0; i<worker_queue.size(); ++i)
		{
			unsigned index=(thread_index+i+1)%worker_queue.size();
			if(worker_queues[index]->try_steal(task))
				return true;
		}
		return false;
	}
public:
	thread_pool(): done(false), joiner(worker_threads)
	{
		unsigned const thread_count=std::thread::hardware_concurrency();
		try
		{
			worker_queues=std::vector<std::unique_ptr<
				work_stealing_queue>> (thread_count);
			for(unsigned i=0; i<thread_count; ++i)
			{
				worker_queues.reset(new work_stealing_queue);
			}
			for(unsigned i=0; i<thread_count; ++i)
			{
				worker_threads.push_back(std::thread(
					&thread_pool::worker_thread, this, i));
			}
		} catch(...)
		{
			done=true;
			throw;
		}
	}
	// other contructors go here <!!SHOULD BE NON-COPYABLE!!>
	~thread_pool()
	{
		done=true;
	}
	template <typename FunctionType>
	std::future<typename std::result_of<FunctionType()>::type>
		submit(FunctionType func)
	{
		typedef typename std::result_of<FunctionType()>::type> 
			result_type;
		std::packaged_task<result_type()> task {std::move(func)};
		std::future<result_type> res=task.get_future();
		if(local_queue)
			local_queue->push(std::move(task));
		else
			global_queue.push(std::move(task));
		return res;
	}
	void run_pending_task()
	{
		function_wrapper task;
		if(try_to_get_from_local_queue(task) ||
		   try_to_get_from_global_queue(task) ||
		   try_to_steal_from_another_worker_queue(task))
		{
			task();
		} 
		else
			std::this_thread::yield();
	}
};
```
refer to task for further explanation.

## Interrupting Threads