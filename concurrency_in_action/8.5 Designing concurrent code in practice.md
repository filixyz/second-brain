This subsection looks at the implementation of parallel version of three functions from the c++ Standard library so to give us a familiar basis on which to build while providing a platform for looking at issues.

## A parallel implementation for `std::for_each`
This is a simple parallel implementation of std library’s `std::for_each`: It assumes it’s the only parallel task running hence relies on `std::thread::hardware_concurrency()`  to determine thread count. It also uses `std::packaged_task` and `std::future` to pass exceptions to it’s caller in cases where the need might arise. Since there is no cumulative result to be returned; the futures sole responsibility is exception propagation. Lastly, given that elements can be processed entirely independently, this implementation uses contiguous blocks to avoid false sharing

Here is the implementation:
```c++
template<typename Iterator, typename Func>
void parallel_for_each(Iterator first, Iterator last, Func f)
{
	unsigned long const length=std::distance(first, last);
	if(!length)
		return;
	unsigned long const min_per_thread=25;
	unsigned long const max_thread=
		(length+min_per_thread-1)/min_per_thread;
	unsigned long const hardware_threads=
		std::thread::hardware_concurrency();
	unsigned long const num_threads=
		std::min(hardware_threads!=0?hardware_threads:2,max_threads);
	unsigned long const block_size=length/num_threads;
	std::vectot<std::future<void>> futures(num_threads-1);
	std::vector<std::thread> threads(num_threads-1);
	join_threads joiner(threads);
	Iterator block_start=first;
	for(unsigned long i=0; i<(num_threads-1); ++i)
	{
		Iterator block_end=block_start;
		std::advaance(block_end, block_size);
		std::packaged_task<void(void)> task(
			[=](){
				std::for_each(block_start, block_end, f);
			}
		);
		futures[i]=task.get_future();
		threads[i]=std::thread(std::move(task));
		block_start=block_end;
	}
	std::for_each(block_start, last, f);
	for(unsigned long i=0; i<(num_threads-1); ++i)
		futures[i].get(); <-------------------------------[1]
}
```
The part of the code at the bottom end tagged `[1]`, the loop that goes through all futures to invoke their `.get()` is to ensure any (or more accurately the first) exception encountered is propagated to the caller. If exception propagation is not important it can be omitted.

Below is another parallel implementation of `std::for_each` is further simplified by relying on`std::async`:
```c++
template<typename Iterator, typename Func>
void parallel_for_each(Iterator first, Iterator last, Func f)
{
	unsigned long const length=std::distance(first, last);
	if(!length)
		return;
	unsigned long const min_per_thread=25;
	if(length<(2*min_per_thread))
		std::for_each(first, last, m);
	else
	{
		Iterator const mid_point=first+length/2;
		std::future<void> first_half=
			std::async(&parallel_for_each<Iterator, Func>,
				first,mid_point, f);
		parallel_for_each(mid_point, last, f);
		first_half.get();
	}
}
```
##  A parallel implementation  of `std::find`
This implementation divides a set of elements into chunks to be searched in parallel. It relies on `std::thread::hardware_concurrency`for it’s thread count. The result is the stored in an `std::promise` variable accessible by all threads then the atomic boolean flag `std::atomic<bool>` (the program uses to signal if a thread has found a match so other threads can cease their execution) is flagged causing other threads to cease their search. If an exception was encountered by a thread (before a result was found) that exception is stored in the promise instead of a match

The Implementation:
```c++
template<typename Iterator, typename MatchType>
Iterator parallel_find(Iterator first, Iterator last, MatchType match)
{
	struct find_element
	{
		void operator()(Iterator begin, Iterator end, MatchType Match,
				        std::promise<Iterator>* result,
					    std::atomic<bool>* done_flag)
		{
			try
			{
				for(;begin!=end && !done_flag->load(); ++begin())
				{
					if(*begin==match)
					{
						result->set_value(begin);
						done_flag->store(true);
						return;
					}
				}
			}
			catch(...)
			{
				try
				{
					result->set_exception(std::current_exception());
					done_flag->store(true);
				}
				catch(...)
				{}
			}
		}
		
	};
	unsigned long const length=std::distance(first, last);
	if(!length)
		return last;
	unsigned long const min_per_thread=25;
	unsigned long const max_threads=
		(length+min_per_thread-1)/min_per_thread;
	unsigned long const hardware_threads=
		std::thread::hardware_concurrency();
	unsigned long const num_threads=
		std::min(hardware_threads!=0?hardware_threads:2, max_threads);
	unsigned long const block_size = length/num_threads;
	std::promise<Iterator> result;
	std::atomic<bool> done_flag(false);
	std::vector<std::thread> threads(num_threads-1);
	{
		join_threads joiner(threads);
		Iterator block_start=first;
		for(unsigned long i=0; i<(num_threads-1); ++i)
		{
			Iterator block_end=block_start;
			std::advance(block_end, block_size);
			threads[i]=std::thread(
				find_element(), block_start, block_end, match,
				&result, &done_flag
			);
			block_start=block_end;
		}
		find_element()(block_start, last, match, &result, &done_flag);
	}
	if(!done_flag.load())
		return last;
	return result.get_future().get();
}
```
Here is another implementation using `std::async` instead of manually managing the threads:
```c++
template<typename Iterator, typename MatchType>
Iterator parallel_find_impl(Iterator first, Iterator last, 
	MatchType match, std::atomic<bool>& done)
{
	try
	{
		unsigned long const length=std::distance(first, last);
		unsigned long const min_per_thread=25;
		if(length<(2*min_per_thread))
		{
			for(;(first!=last)&&!done.load(); ++first)
				if(*first==match)
				{
					done=true;
					return first;
				}
			return last;
		}
		else
		{
			Iterator const mid_point=first+(length/2);
			std::future<Iterator> async_result=
				std::async(&parallel_find_impl<Iterator, MatchType>,
						   mid_point, last, match, std::ref(done));
			Iterator const direct_result=
				parallel_find_impl(first, mid_point, match, done);
			return (direct_result==mid_point)?
				async_result.get() : direct_result;
		}
	}
	catch(...)
	{
		done=true;
		throw;
	}
}
template<typename Iterator, typename MatchType>
Iterator parallel_find(Iterator first, Iterator last, MatchType match)
{
	std::atomic<bool> done(false);
	return parallel_find_impl(first, last, match, done);
}
```

## A parallel implementation of `std::partial_sum`
consult the text for the explanation if confused in the future, the code is self explanatory though once you understand  `std::partial_sum`:
```c++
template<typename Iterator>
void parallel_partial_sum(Iterator first, Iterator last)
{
	typedef typename Iterator::value_type value_type;
	struct process_chunk
	{
		void operator(Iterator first, Iterator last, 
			std::future<value_type>* previous_end_value,
			std::promise<value_type>* end_value)
		{
			try
			{
				Iterator end=last;
				++end;
				std::partial_sum(begin, end, begin);
				if(previous_end_value)
				{
					value_type& addend=previous_end_value->get();
					*last+=addend;
					if(end_value)
						end_value->set_value(*last);
					std::for_each(first, last, [addend](value_type& item)
						{
							item+=addend;
						});
				}
				else if(end_value)
					end_value->set_value(*last);
			}
			catch(...)
			{
				if(end_value)
					end_value->set_exception(std::current_exception());
				else
					throw;
			}
		}
	};
	unsigned long const length=std::distance(first, last);
	if(!length)
		return;
	unsigned long const min_per_thread=25;
	unsigned long const max_thread=
		(length+min_per_thread-1)/min_per_thread;
	unsigned long const hardware_threads=
		std::thread::hardware_concurrency();
	unsigned long const num_threads=
		std::min(hardware_threads!=0?hardware_threads:2, max_threads);
	unsigned long const block_size=length/num_threads;
	typedef typename Iterator::value_type value_type;
	std::vector<std::thread> threads(num_threads-1);
	std::vector<std::promise<value_type>> end_values(num_threads-1);
	std::vector<std::future<value_type>> previous_end_values;
	previous_end_values.reserve(num_threads-1);
	join_threads joiner(threads);
	Iterator block_start=first;
	for(unsigned long i=0; i<(num_threads-1); ++i)
	{
		Iterator block_last=block_start;
		std::advance(block_last, block_size-1);
		threads[i]=std::thread(process_chunk(), block_start, block_last
							   (i!=0)?&previous_end_values[i-1]:0,
								&end_values[i]);
		block_start=block_last;
		++block_start;
		previous_end_values.push_back(end_values[i].get_future());
	}
	Iterator final_element=block_start;
	std::advance(final_element, std::distance(block_start, last)-1);
	process_chunk(block_start, final_element,
				  (num_threads>1)?&previous_end_values.back():0,
				  0);
}
```

### Implementing the incremental pairwise algorithm for partial sums
This implementation uses a lockstep approach for each step in it’s algorithm and this is easily achieved using a barrier, so here is a simple spin-lock barrier class implementation and following it is the incremental pairwise algorithm for partial sums:
```c++
class barrier
{
	std::atomic<unsigned> count;
	std::atomic<unsigned> spaces;
	std::atomic<unsigned> generation;
public:
	explicit barrier(unsigned seats):
		count(seats), spaces(seats), generation(0)
	{}
	void wait()
	{
		unsigned const my_generation=generation;
		if(!--spaces)
		{
			spaces=count;
			++generation;
		}
		else
		{
			while(my_generation==generation.load())
				std::this_thread::yield()
		}
	}
	void done_waiting()
	{
		--count;
		if(!--spaces)
		{
			spaces=count.load();
			++generation;
		}
	}
};
template<typename Iterator>
void parallel_partial_sum(Iterator first, Iterator last)
{
	typedef typename Iterator::value_type value_type;
	struct process_element
	{
		void operator()(Iterator first, Iterator last,
						std::vector<value_type>& buffer,
						unsigned i, barrier& b)
		{
			value_type& ith_element=*(first+i);
			bool  update_source=false;
			
			for(unsigned step=0,stride=1; stride<=i; ++step,stride*=2)
			{
				value_type const& source=(step%2)?
					buffer[i]:ith_element;
				value_type const& dest=(step&2)?
					ith_element:buffer[i];
				value_type const& addend=(step&2)?
					buffer[i-stride]:(first+i-stride);
				dest=source+addend;
				update_source=!(step%2);
				b.wait()
			}
			if(update_source)
				ith_element=buffer[i];
			b.done_waiting();
		}
	};
	unsigned long const length=std::distance(first, last);
	if(length<=1)
		return;
	std::vector<value_type> buffer(length);
	barrier b(length);
	std::vector<std::thread> threads(length-1);
	join_threads joiner(threads);
	Iterator block_start=first;
	for(unsigned long i=0; i<(length-1); ++i)
	{
		threads[i]=std::thread(process_element(), first, last,
			                   std::ref(buffer), i, std::ref(b));
	}
	process_element()(first, last, buffer, length-1, b);
}
```
