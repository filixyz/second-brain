 This chapter focuses on how to apply the tools and information gathered from previous chapter to build concurrent code
## Techniques for dividing work between threads
There were a few techniques outlined in the section, these techniques are:
1) Dividing data between threads before processing begins
2) Dividing work recursively, and
3) Dividing work based on task type

### Dividing data between threads before processing begins
This is the simplest paradigm for dividing computations in parallel, in this techniques the data structure and its elements are examined, then the first a set of $N$ elements in the data structure is allocated to one thread and so on. 

No matter how the elements are divided, each thread begin processing on it’s allotted elements with no communication with other threads, until processing is complete.

In summary the tasks are splits into parallel tasks, the worker threads run this task independently then the results are combined in a final reduction step. Although this approach is powerful, it cannot be applied to every kind of computation.

### Dividing data recursively
This technique operates on the logic that one can maximum the parallel computation potential of a concurrent program if in cases where operations are performed recursively (running the same code multiple times on the same processor but on different stack spaces) those operations, instead, (some of them most ideally) are performed in parallel with each other (running the same code multiple times on different processors each, usually, with very limited number of recursively invoked stack spaces). 

This approach works when the work to be divided can be done so with each subset completely capable of being processed independently to one another, just like a basic recursive call.

So the summary of this approach is, instead of making regular sequential recursive calls, make (some) parallel recursive calls, because if you think about it recursive and asynchronous calls follow the same semantics, the latter just runs on a separate thread and just like regular recursive calls, a channel , if needed, for propagating the result of each subset call to the entirety of the program should be though of.

An example of this can be applied to the quicksort algorithm, instead of invoking two recursive calls for for the higher and lower chunks, `std::async` can be used to spawn an asynchronous call for the lower chunk at each stage. Using `std::async` instead of spawning threads willy-nilly for each stage is highly important for performance, this is because relying on `std::async` is relying on the c++ thread library to decide when to run a task on a new thread or synchronously, this means c++ helps manage the program in correlation with the threading capability of your platform being executed upon, not managing your threads could lead excessive threads, larger than what the system could sustain which leads to problems, like over-subscription, task switching, cache ping pongs and overall degradation in execution prowess.

one could manage their own threads with `std::thread::hardware_concurrency()` also.

Here is an implementation of parallel quick-sort following all we had discussed:
```c++
template<typename T>
struct sorter
{
	struct chunk_to_sort
	{
		std::list<T> data;
		std::promise<std::list<T>> promise;
	};	
	thread_safe_stack<chunk_to_sort> chunks;
	std::vector<std::threads> threads;
	unsigned const max_thread_count;
	std::atomic<bool> end_of_data;
	
	sorter():
		max_thread_count(std::thread::hardware_concurrency()-1),
		end_of_data(false)
	{}
	~sorter()
	{
		end_of_data=true;
		for(unsigned i=0;i<max_thread_count;++i)
			threads[i].join();
	}
	void try_sort_chunk()
	{
		boost::shared_ptr<chunk_to_sort> chunk=chunk.pop();
		if(chunk)
			sort_chunk(chunk);
	}
	std::list<T> do_sort(std::list<T>& chunk_data)
	{
		if(chunks_data.empty())
			return chunk_data;
		std::list<T> result;
		result.splice(result.begin(), chunk_data, chunk_data.begin());
		T const& partition_val=*result.begin();
		
		typename std::list<t>::iterator divide_point=
			std::partition(
				chunk_data.begin(), chunk_data.end()),
				[&](T const& val){return val<partition_val;}
			);
			
		chunk_to_sort new_lower_chunk;
		new_lower_chunk.data.splice(new_lower_chunk.data.end(),
									chunk_data, chunk_data.begn(),
									divide_point);
		std::future<std::list<T>> new_lower=
			new_lower_chunk.promise.get_future();
		chunks.push(std::move(new_lower_chunk));
		if(threads.size()<max_thread_count)
			threads.push_back(std::thread(&sorter<T>::sort_thread, this));
		std::list<T> new_higher(do_sort(chunk_data));
		result.splice(result.end(), new_higher);
		while(new_lower.wait_for(std::chrono::seconds(0))!=
			std::future_status::ready)
		{
			try_sort_chunk();
		}
		result.splice(result.begin(), new_lower.get());
		return result;
	}
	void sort chunk(boost::shared_ptr<chunk_to_sort> const& chunk)
	{
		chunk->promise.set_value(do_sort(chunk->data));
	}
	void sort_thread()
	{
		while(!end_of_data)
		{
			try_sort_chunk();
			std::this_thread::yield();
		}
	}
};

template<typename T>
std::list<T> parallel_quick_sort(std::list<T> input)
{
	if(input.empty())
		return input;
	sorter<T> s;
	return s.do_sort(input);
}
```
Both dividing data before processing and dividing it recursively presume that the data itself id fixed beforehand and your looking for ways to divide it. This doesn’t work for all case, in cases where data is dynamically generated or is coming from external input it would make sense to divide work by task type rather than dividing based on data.


### Dividing work by task type
An alternative to dividing work as chunks for threads is to make threads specialists, where each performs a distinct task and performs that task well. Threads following this paradigm may or may not perform on the same data, but if they do its for a different purpose.

#### Dividing work by task type to separate concerns
Making threads specialists can help design more modular code, given that each thread performs a different task compared to it’s neighbor. Specialist thread are designed to manage different concerns, processes and activities in the grand scheme that is the entirety of the program.

In this paradigm one thread could be a specialist that receives packets from a network, while another thread is responsible for applying the business logic of the program to those packets, while another thread is responsible for updating the GUI and another thread sends data packets over a network if needed, this separation and delegation of activities to different specialist threads help separate concerns while making the code both easier to write and understand.

if we had opted to executing all individually separable tasks (that compose a typical program), all in a single thread this would mean said thread would have to perform a little bit of everything and still save states in situations where the overall state should be changed.

For example if a program is composed of multiple tasks each with their own states that the program, holistically, must be cognizant of at all times. A single threaded application would have to somehow perform all this tasks sequentially and in the event that current task/state needs to be switched, the current task/state would have to store a snapshot of itself (to be retrieved when next it’s its turn to execute again), prepare to hand over execution to the next state and terminate while handing over.

This not only makes the program complex to think about but it also increases latency in response in situations where the user demands for the output of an input that the current state of the program at that moment is incapable of handling, the program after intercepting this request would have to store the details of it’s current state, terminate that state, and load the task/state that is capable of handle whatever the user just demanded.

 When dividing work across threads by teak type, you don’t have to limit yourself to completely isolated cases, if multiple sets of input data require the same sequence of operations to be applied, you can divide the work so each thread performs one stage from the overall sequence.


#### Dividing a sequence of tasks between threads.
if a task consists of applying the same sequence of operations on a number of independent data items, on can use a ***Pipeline*** to exploit the concurrency capabilities of the hardware the task to be executed upon.

The semantics follows the analogy of a physical pipeline; data flows in at one end through a series of operations (pipes) and out the other end.

To divide work this way, a separate thread is created for each stage of the pipeline—one thread for each of the operations in the sequence. When the operation is completed, the data element is placed in a queue to be picked up by the next thread (pipe).

Pipelines are also good when each operation in the sequence is time-consuming; by dividing by tasks rather than data, the performance profile of the program is altered: The system now resembles that of products placed in a conveyor belt in a factory in which the conveyor transports each elements across individual stations and those stations perform their duty in the conveyor line.

This way instead of output being released as batches of elements,  all released at one once as it is when threads are divided by data, when threads are divided by task as pipelines, output is released as a sequential order of single elements in which after the first element the next element is released after the total time it takes for processing a single element on the fastest processing thread/task has elapsed.

in summary if we have an operation composed of 4 tasks to be executed on 4 cores, and each task takes 3 seconds to complete, if we divide threads by data, in 12 seconds we would have 4 elements released as a batch (all at once, no interleaved spacing within the release of each element), and in 24 seconds we would have 8 elements released as a batch and so on.

But if we divide by task, with each core handling each task, meaning after the first 3 seconds the first element moves to the next task hence next core/thread for processing so the previous thread can begin working on the next elements, and this continues happening as each element propagate through the cores/task/threads. Meaning when the first elements is at the last core/task the next element is being processed at the core/task prior to the first elements and when the first elements is done with it’s processing the next element enters the last core/task $-3seconds$ from the time the first elements just became fully processed, HENCE, processing of individual elements still take their allotted $12seconds$ but output is spaced $3seconds$ apart.

This paradigm makes it possible to get 1 item processed every 3 seconds, rather than having items processing in batches of 4 every 12 seconds

This paradigm might be slower in processing but it proves useful when latency in output is an undesirable trait to have, like video streaming through a network or online gaming

## Factors affecting the performance of concurrent code
There are numerous factors that affect the performance of concurrent code, the text outlines the majority of them, which are:
1) Processor count
2) Data contention and cache ping pong
3) False sharing
4) Data proximity
5) Over-subscription and excessive task switching

### How many processors?
The number and structure of processors can vastly impact the performance of multi-threaded applications. 

When developing with aim of  leveraging the concurrency capabilities of the hardware in mind, it is recommended to spawn threads equal to the actual threading capabilities of the hardware, for example in a 16 core or 4 quad-core processor, which can normally churn out 16 threads, our application should leverage on that and try to spawn as much threads but not more than the maximum threading capacity, if we spawn more than 16 threads the processors might experience excessive task switching and over-subscription which is an undesirable trait. 

On the other hand if we spawn less that 16 threads; we leave processing power on the table—untapped (unless the system is running other applications too)

To allow applications scale it’s number of threads with the number of threads the hardware can run concurrently, c++ provides `std::thread::hardware_concurrency()`.

`std::thread::hardware_concurrency` should be used with care because the function doesn’t take into account other threads running on the system unless we explicitly share that information, because of this if multiple application scale themselves using `hardware_concurrency`, there would be massive contention on the threads, causing excessive over-subscription. 

if we choose not to manage our threads ourselves in order not to think of how to scale maximally without causing excessive task switching if other applications are also trying to scale maximally using `std::thread::hardware_concurrency`, we can rely on `std::async` to spawn asynchronous task since it takes into account other applications running on the hardware and the threads they occupy, by doing this we hand over the responsibility of managing the threads to the c++ library.

### Data contention and cache ping-pong
As the number of processors increases so does the likelihood of a performance problem, multiple processors trying to access the same data.

If two threads executing concurrently on separate threads require the value of some data; usually that is not a problem; all that is required is the value to be copied to their respective caches then the threads can continue/begin their processing, but in the event that a thread modifies the data, this change could cause the other thread to pause in it’s processing while the change propagates through memory hardware to arrive at the threads processor caches, where once arrived the thread can now continue it’s processing.

This propagation event is usually an expensive and slow operation equivalent to a many hundred  individual instructions

consider the following code:
```c++
std::atomic<unsigned> counter(0);
void processing_loop()
{
	while(counter.fetch_add(1, std::memory_order_relaxed)<100000000)
		do_something();
}
```
counter is global variable so any threads that calls `processing_loop` is able to increment it, now given that `fetch_add` is a read-modify-write operations it requires whatever thread invoking it obtain the most recent value of counter, update that value and publish that update to other threads, although we’ve made the operation a relaxed one meaning it doesn’t need any synchronization with other threads, the memory ordering still doesn’t change the fact that `fetch_add` requires an updated version of counter whenever it’s invoked. 

So in the scenario that two threads are executing `processing_loop`, the data for counter must be passed back and forth between the two processors and their corresponding cache  so that each processor has the latest value of counter when it does the increment.

if the number of threads go up (each (subsets) on separate processors) the contention for the latest value of counter goes up and processors might find themselves waiting for each other; one processor is ready to update the value, but another is currently doing that.

This situation is called *high contention*. If the processor rarely have to wait for each other that is *low contention*. 

In a loop like the one above where counter would have to passed to and fro numerous caches this situation is known as *cache ping pong*, this can impact performance seriously, because while a processor waits for it’s cache to be updated it can do no work at that moment even if it has a list of meaningful work (threads) to do.

cache ping pongs not only happens with atomic variables, it can also happen when there is (high) contention in acquiring mutexes. The effects of contention with mutexes are different to that of atomic variables; this is because mutexes inherently serializes threads at the operating system level rather than at processor level so if you have enough threads ready to run the operating system can schedule another thread to run while one thread is waiting for a mutex, whereas a processor stall (like the one with atomic variables) prevent any threads from running on that processor.

### False sharing
False sharing is a situation in which multiple threads (≤2) share the same cache line but do not share any data in that cache line between themselves.

Processor caches do not generally deal with memory locations; instead they deal with blocks of memory called *cache lines*, these blocks are typically 32 or 64 bytes in size.

Because caches hardware only deals with cache-line sized blocks of memory, adjacent data in memory will be in the same cache line, sometimes this is good if a processor thread requires access to multiple data that are stored closely with each other in memory, then they would be easily picked up by the same cache line.

But in the case that the data in the cache lines are unrelated and need to be accessed by different threads on different processors this can prove to impact performance.

say we have an array of `int` with separate threads responsible for managing the update and access of each entry in the array, if a thread requires the value/memory for its entry, the cache line goes to the memory hardware, picks up a  cache-line sized block from memory that contains the value for that entry, along with the value of other entries, that the thread does not need mind you, and supplies that cache line to the processor of the thread. Now say concurrently another thread requires the value for it’s own entry, and that entry  was included in the cache line sized block the cache line previously picked up, ownership of the cache line would now have to be relinquished to this new thread and the cache line transported to its processor. In this scenario both threads and their processors are now entangled in a game of cache-ping-pong when they do not share any data among themselves, this situation is known as *false sharing*

The solution here is to structure data so that items to be accessed by the same thread are closed together in memory (and so more likely to be in the same cache line), whereas those accessed by separate threads are far apart in memory and thus more likely to be in separate cache lines.


C++17 provides `std::hardware_destructive_inteference`  in the `<new>` header, this function specifies the maximum number of consecutive bytes that may be subjected to false sharing for the compilation target. If one can ensure that their data is at least this number of bytes apart, then there will be no false sharing.

### Data Proximity
Structuring data far apart can be prove useful when avoiding false sharing, but this introduces a new problem if a single thread requires access to multiple data that are not adjacent in memory

if data required by a single thread is spaced out in memory they are likely to lie in separate cache lines and on the flip side if they are more closer in memory they are more likely to lie in the same cache-line, in the event that the former is the situation, this can place a lot of pressure on the caches, to load more cache lines from memory onto the processor cache.

The pressure on the cache increases memory access latency which reduces performance, and this scenario is a more humble version of what could be since we’ve only highlighted this issue in the perspective of a single thread. In a case where the hardware processor experience task switching due to numerous threads, the performance cost will be immense due to possible cache misses, increased pressure on each processor cache and cache-lines reloads all just to avoid false sharing.

C++17 specifies the constant `std::hardware_constructive_interference_size`, also in in the `<new>` header, which is the maximum number of consecutive bytes guaranteed to be on the same cache-line (if suitable aligned), if one can fir data that is needed together within this number of bytes, it will potentially reduce the number of cache misses.

### Over-subscription and excessive task switching
In multi-threaded systems it’s normal to have more threads than cores, except in massively parallel systems. This is okay because most threads are waiting on I/0, blocked on mutexes or condition variables so, most of the time the actual active threads are what the system can handle normally. 

Having extra threads enables applications perform useful work rather than sitting idle while threads wait.

This isn’t always a good thing, especially if the treads are so numerous the CPU has to resort to task switching and over subscription, Over-subscription can arise when there is a task that repeatedly spawns new threads without limits.

If we spawning too many threads due to data division, limiting the number of worker threads can prevent this issue but if the over-subscription is due to natural division of work, there is not a lot we can do to ameliorate the issue.